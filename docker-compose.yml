services:
  # Redis for persistent job storage
  redis:
    image: redis:7-alpine
    container_name: scraper-redis
    ports:
      - "${REDIS_PORT:-6379}:${REDIS_PORT:-6379}"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory ${REDIS_MAX_MEMORY:-512mb} --maxmemory-policy ${REDIS_MAX_MEMORY_POLICY:-allkeys-lru}
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - scrapee-network

  # Optional: Redis Commander for Redis management UI
  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: redis-commander
    ports:
      - "${REDIS_COMMANDER_PORT:-8081}:${REDIS_COMMANDER_PORT:-8081}"
    environment:
      - REDIS_HOSTS=local:redis:${REDIS_PORT:-6379}
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - scrapee-network

  # Main scraper application (Arachne)
  arachne:
    build: ./arachne
    container_name: scraper-app
    # Chrome requires adequate shared memory for rendering
    shm_size: '2gb'
    ports:
      - "${ARACHNE_PORT:-8080}:${ARACHNE_PORT:-8080}"
    environment:
      - SCRAPER_REDIS_ADDR=${ARACHNE_REDIS_ADDR:-redis:6379}
      - SCRAPER_REDIS_DB=${ARACHNE_REDIS_DB:-0}
      - SCRAPER_ENABLE_METRICS=${ARACHNE_ENABLE_METRICS:-true}
      - SCRAPER_ENABLE_LOGGING=${ARACHNE_ENABLE_LOGGING:-true}
      - SCRAPER_LOG_LEVEL=${ARACHNE_LOG_LEVEL:-info}
      - SCRAPER_MAX_CONCURRENT=${ARACHNE_MAX_CONCURRENT:-5}
      - SCRAPER_REQUEST_TIMEOUT=${ARACHNE_REQUEST_TIMEOUT:-120s}
      - SCRAPER_TOTAL_TIMEOUT=${ARACHNE_TOTAL_TIMEOUT:-180s}
      - SCRAPER_USE_HEADLESS=${ARACHNE_USE_HEADLESS:-true}
      - SCRAPER_HEADLESS_NO_SANDBOX=${ARACHNE_HEADLESS_NO_SANDBOX:-true}
      - SCRAPER_USER_AGENT=${ARACHNE_USER_AGENT:-Mozilla/5.0 (compatible; PortfolioBot/1.0)}
      - SCRAPER_RATE_LIMIT=${ARACHNE_RATE_LIMIT:-2}
      - SCRAPER_RATE_LIMIT_WINDOW=${ARACHNE_RATE_LIMIT_WINDOW:-1s}
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:8080/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    volumes:
      - ./arachne/scraping_results:/app/results
      - ./arachne/data:/app/data
    networks:
      - scrapee-network

  # AI Backend service
  ai-backend:
    build: ./ai-backend
    container_name: ai-backend
    ports:
      - "${AI_BACKEND_PORT:-3001}:${AI_BACKEND_PORT:-3001}"
    environment:
      - NODE_ENV=${NODE_ENV:-production}
      - PORT=${AI_BACKEND_PORT:-3001}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ARACHNE_URL=http://arachne:${ARACHNE_PORT:-8080}
      - TURNSTILE_REQUIRED=${TURNSTILE_REQUIRED:-false}
    depends_on:
      arachne:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - scrapee-network

  # Frontend service
  frontend:
    build: ./frontend
    container_name: frontend
    ports:
      - "${FRONTEND_PORT:-3000}:${FRONTEND_PORT:-3000}"
    environment:
      - NEXT_PUBLIC_ARACHNE_API_URL=http://arachne:${ARACHNE_PORT:-8080}
      - ARACHNE_API_URL=http://arachne:${ARACHNE_PORT:-8080}
      - AI_BACKEND_URL=http://ai-backend:${AI_BACKEND_PORT:-3001}
      - NEXT_PUBLIC_APP_URL=http://localhost:${FRONTEND_PORT:-3000}
    depends_on:
      - arachne
      - ai-backend
    restart: unless-stopped
    networks:
      - scrapee-network

volumes:
  redis_data:
    driver: local

networks:
  scrapee-network:
    driver: bridge
